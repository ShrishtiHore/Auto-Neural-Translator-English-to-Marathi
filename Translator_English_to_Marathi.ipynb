{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translator_English_to_Marathi.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_bJ0RazZuni"
      },
      "source": [
        "#Neural Machine Translation (NMT) - Translating English to Marathi.\r\n",
        "\r\n",
        "Machine translation refers to translating phrases across languages using deep learning and specifically with RNN (Recurrent Neural Nets). Most of these are complex system that is they are combined system of various algorithms.\r\n",
        "But, at its core, NMT uses sequence-to-sequence ( seq2seq ) RNN cells. Such models could be character level but word level models remain common."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F32yt_b5bnr6"
      },
      "source": [
        "**Step 1: Downloading Datset and Libraries**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HwzrIQObx_m"
      },
      "source": [
        "**(a) Importing the Libraries**\r\n",
        "IMport Tensorflow and Keras. From Keras, import modules that build NN layers, preprocess data and construct LSTM models.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOukyETtZSm1"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\r\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt-GQHGkdECV"
      },
      "source": [
        "**(b) Reading the Data**\r\n",
        "The given dataset contains more than 30k pairs of English-Marathi phrases.Just donwload the dataset and unzip it and read it using Pandas.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eChUK3XWdEmV",
        "outputId": "26f9b029-226e-4a9d-b82b-cce506cc27d0"
      },
      "source": [
        "!wget http://www.manythings.org/anki/mar-eng.zip -O mar-eng.zip\r\n",
        "!unzip mar-eng.zipA"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-06 13:54:17--  http://www.manythings.org/anki/mar-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 172.67.173.198, 104.21.55.222, 2606:4700:3036::ac43:adc6, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|172.67.173.198|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1315254 (1.3M) [application/zip]\n",
            "Saving to: ‘mar-eng.zip’\n",
            "\n",
            "mar-eng.zip         100%[===================>]   1.25M  1.23MB/s    in 1.0s    \n",
            "\n",
            "2021-02-06 13:54:18 (1.23 MB/s) - ‘mar-eng.zip’ saved [1315254/1315254]\n",
            "\n",
            "Archive:  mar-eng.zip\n",
            "replace mar.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: mar.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "-75oIwoQdFlu",
        "outputId": "62c1c981-c176-46e2-8154-6b1d8d97afff"
      },
      "source": [
        "lines = pd.read_table('mar.txt', names=['eng', 'mar'])\r\n",
        "lines.reset_index(level=0, inplace=True)\r\n",
        "lines.rename(columns={'index' : 'eng', 'eng' : 'mar', 'mar' : 'c'}, inplace = True)\r\n",
        "lines = lines.drop('c', 1)\r\n",
        "lines = lines.iloc[ 10000 : 20000 ] \r\n",
        "lines.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>mar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10000</th>\n",
              "      <td>She's a sweet girl.</td>\n",
              "      <td>गोड मुलगी आहे ती.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>Show me an example.</td>\n",
              "      <td>मला एक उदाहरण दाखवा.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10002</th>\n",
              "      <td>Show me an example.</td>\n",
              "      <td>मला एक उदाहरण दाखव.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10003</th>\n",
              "      <td>Show me everything.</td>\n",
              "      <td>मला सगळं दाखव.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10004</th>\n",
              "      <td>Show me everything.</td>\n",
              "      <td>मला सगळं दाखवा.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       eng                   mar\n",
              "10000  She's a sweet girl.     गोड मुलगी आहे ती.\n",
              "10001  Show me an example.  मला एक उदाहरण दाखवा.\n",
              "10002  Show me an example.   मला एक उदाहरण दाखव.\n",
              "10003  Show me everything.        मला सगळं दाखव.\n",
              "10004  Show me everything.       मला सगळं दाखवा."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AzfSAhiecH8"
      },
      "source": [
        "**Step 2: Preparing input data for the Encoder**\r\n",
        "The Encoder model will be fed input data which are preprocessed English sentences. The preprocessing is done as follows:\r\n",
        "\r\n",
        "1.   Tokenizing the English sentences from eng_lines.\r\n",
        "2.   Determining the maximum length of the English sentence that's max_input_length.\r\n",
        "3.   Padding the tokenized_eng_lines to the max_input_length.\r\n",
        "4.   Determining the vocabulary size ( num_eng_tokens ) for English words.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCyOzi8vdFiV",
        "outputId": "9ddd70a0-8948-4269-d15c-200ad0016579"
      },
      "source": [
        "eng_lines = list()\r\n",
        "for line in lines.eng:\r\n",
        "  eng_lines.append( line )\r\n",
        "\r\n",
        "tokenizer = preprocessing.text.Tokenizer()\r\n",
        "tokenizer.fit_on_texts(eng_lines)\r\n",
        "tokenized_eng_lines = tokenizer.texts_to_sequences(eng_lines)\r\n",
        "\r\n",
        "length_list = list()\r\n",
        "for token_seq in tokenized_eng_lines:\r\n",
        "  length_list.append(len(token_seq))\r\n",
        "max_input_length = np.array(length_list).max()\r\n",
        "print( 'English max length is {}'.format( max_input_length ))\r\n",
        "\r\n",
        "padded_eng_lines = preprocessing.sequence.pad_sequences( tokenized_eng_lines , maxlen=max_input_length , padding='post' )\r\n",
        "encoder_input_data = np.array( padded_eng_lines )\r\n",
        "print('Encoder input data shape -> {}'.format(encoder_input_data.shape))\r\n",
        "\r\n",
        "eng_word_dict = tokenizer.word_index\r\n",
        "num_eng_tokens = len(eng_word_dict)+1\r\n",
        "print('Number of Encoder tokens = {}'.format(num_eng_tokens))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English max length is 7\n",
            "Encoder input data shape -> (10000, 7)\n",
            "Number of Encoder tokens = 2424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsnXOmgnh-R0"
      },
      "source": [
        "**Step 3: Preparing Input Data for the Decoder (decoder_input_data)**\r\n",
        "\r\n",
        "The decoder model will be fed the preprocessed Marathi lines. The preprocessing steps are similar to the ones which are above. This one step is carried out before the other steps.\r\n",
        "\r\n",
        "\r\n",
        "*   Append <START> tag at the first position in each Marathi sentence.\r\n",
        "*  Append <END> tag at the last position in each Marathi sentence.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omgFsEHjdFfP",
        "outputId": "a779d57c-1a10-445b-cf66-f1f13101b9b4"
      },
      "source": [
        "mar_lines = list()\r\n",
        "for line in lines.mar:\r\n",
        "    mar_lines.append( '<START> ' + line + ' <END>' )  \r\n",
        "\r\n",
        "tokenizer = preprocessing.text.Tokenizer()\r\n",
        "tokenizer.fit_on_texts( mar_lines ) \r\n",
        "tokenized_mar_lines = tokenizer.texts_to_sequences( mar_lines ) \r\n",
        "\r\n",
        "length_list = list()\r\n",
        "for token_seq in tokenized_mar_lines:\r\n",
        "    length_list.append( len( token_seq ))\r\n",
        "max_output_length = np.array( length_list ).max()\r\n",
        "print( 'Marathi max length is {}'.format( max_output_length ))\r\n",
        "\r\n",
        "padded_mar_lines = preprocessing.sequence.pad_sequences( tokenized_mar_lines , maxlen=max_output_length, padding='post' )\r\n",
        "decoder_input_data = np.array( padded_mar_lines )\r\n",
        "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\r\n",
        "\r\n",
        "mar_word_dict = tokenizer.word_index\r\n",
        "num_mar_tokens = len( mar_word_dict )+1\r\n",
        "print( 'Number of Marathi tokens = {}'.format( num_mar_tokens))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Marathi max length is 11\n",
            "Decoder input data shape -> (10000, 11)\n",
            "Number of Marathi tokens = 4830\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XqqwS50i19C"
      },
      "source": [
        "**Step 4: Preparing Target Data for the Decoder (decoder_target_data)\r\n",
        "We take a copy of tokenized_mar_lines and modify it like this.\r\n",
        "\r\n",
        "1. We remove the <start> tag which we appended earlier. Hence, the word ( which is <start> in this case ) will be removed.\r\n",
        "2. Convert the padded_mar_lines ( ones which do not have <start> tag ) to one-hot vectors.\r\n",
        "\r\n",
        "`So [ '<start>' , 'hello' , 'world' , '<end>' ]`\r\n",
        "will become:\r\n",
        "` [ 'hello' , 'world' , '<end>' ]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czy93MdwdFcG",
        "outputId": "9c740c01-c899-4acb-b892-4e56088054e7"
      },
      "source": [
        "\r\n",
        "decoder_target_data = list()\r\n",
        "for token_seq in tokenized_mar_lines:\r\n",
        "    decoder_target_data.append( token_seq[ 1 : ] ) \r\n",
        "    \r\n",
        "padded_mar_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\r\n",
        "onehot_mar_lines = utils.to_categorical( padded_mar_lines , num_mar_tokens )\r\n",
        "decoder_target_data = np.array( onehot_mar_lines )\r\n",
        "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder target data shape -> (10000, 11, 4830)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMdw7OcNo2Kd"
      },
      "source": [
        "**Step 5: Defining the Encoder-Decoder Model**\r\n",
        "\r\n",
        "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\r\n",
        "*   2 Input Layers : One for encoder_input_data and another for decoder_input_data.\r\n",
        "*   Embedding layer : For converting token vectors to fix sized dense vectors. ( Note :  Don't forget the mask_zero=True argument here )\r\n",
        "*   LSTM layer : Provide access to Long-Short Term cells.\r\n",
        "\r\n",
        "Working : \r\n",
        "\r\n",
        "1.   The encoder_input_data comes in the Embedding layer (  encoder_embedding ). \r\n",
        "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( h and c which are encoder_states )\r\n",
        "3.   These states are set in the LSTM cell of the decoder.\r\n",
        "4.   The decoder_input_data comes in through the Embedding layer.\r\n",
        "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce sequences.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oibt-fQ_o1u6",
        "outputId": "64d86df0-abb3-4f7a-af22-5622a8e164bf"
      },
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\r\n",
        "encoder_embedding = tf.keras.layers.Embedding( num_eng_tokens, 256 , mask_zero=True ) (encoder_inputs)\r\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 128 , return_state=True  )( encoder_embedding )\r\n",
        "encoder_states = [ state_h , state_c ]\r\n",
        "\r\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\r\n",
        "decoder_embedding = tf.keras.layers.Embedding( num_mar_tokens, 256 , mask_zero=True) (decoder_inputs)\r\n",
        "decoder_lstm = tf.keras.layers.LSTM( 128 , return_state=True , return_sequences=True)\r\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\r\n",
        "decoder_dense = tf.keras.layers.Dense( num_mar_tokens , activation=tf.keras.activations.softmax ) \r\n",
        "output = decoder_dense ( decoder_outputs )\r\n",
        "\r\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\r\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\r\n",
        "\r\n",
        "model.summary()\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 256)    620544      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    1236480     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 128), (None, 197120      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 128),  197120      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 4830)   623070      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,874,334\n",
            "Trainable params: 2,874,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP5cpOGOqFVg"
      },
      "source": [
        "**Step 6: Train the Model**\r\n",
        "We train the model for a number of epochs with RMSprop optimizer and categorical crossentropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BStp6FimdFZX",
        "outputId": "83ad8d09-fe1f-46a6-b707-4dec8a494341"
      },
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250, epochs=50 ) \r\n",
        "model.save( 'model.h5' ) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "40/40 [==============================] - 12s 45ms/step - loss: 3.8511\n",
            "Epoch 2/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 2.7430\n",
            "Epoch 3/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 2.5421\n",
            "Epoch 4/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 2.3900\n",
            "Epoch 5/50\n",
            "40/40 [==============================] - 2s 45ms/step - loss: 2.2727\n",
            "Epoch 6/50\n",
            "40/40 [==============================] - 2s 45ms/step - loss: 2.1947\n",
            "Epoch 7/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 2.1120\n",
            "Epoch 8/50\n",
            "40/40 [==============================] - 2s 45ms/step - loss: 2.0239\n",
            "Epoch 9/50\n",
            "40/40 [==============================] - 2s 45ms/step - loss: 1.9477\n",
            "Epoch 10/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.8837\n",
            "Epoch 11/50\n",
            "40/40 [==============================] - 2s 45ms/step - loss: 1.8007\n",
            "Epoch 12/50\n",
            "40/40 [==============================] - 2s 45ms/step - loss: 1.7492\n",
            "Epoch 13/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.6824\n",
            "Epoch 14/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.6217\n",
            "Epoch 15/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.5685\n",
            "Epoch 16/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.5124\n",
            "Epoch 17/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.4587\n",
            "Epoch 18/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.4060\n",
            "Epoch 19/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.3568\n",
            "Epoch 20/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.3013\n",
            "Epoch 21/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.2590\n",
            "Epoch 22/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.2103\n",
            "Epoch 23/50\n",
            "40/40 [==============================] - 2s 43ms/step - loss: 1.1558\n",
            "Epoch 24/50\n",
            "40/40 [==============================] - 2s 45ms/step - loss: 1.1174\n",
            "Epoch 25/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 1.0729\n",
            "Epoch 26/50\n",
            "40/40 [==============================] - 2s 43ms/step - loss: 1.0344\n",
            "Epoch 27/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.9928\n",
            "Epoch 28/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.9531\n",
            "Epoch 29/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.9163\n",
            "Epoch 30/50\n",
            "40/40 [==============================] - 2s 45ms/step - loss: 0.8712\n",
            "Epoch 31/50\n",
            "40/40 [==============================] - 2s 45ms/step - loss: 0.8374\n",
            "Epoch 32/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.8039\n",
            "Epoch 33/50\n",
            "40/40 [==============================] - 2s 43ms/step - loss: 0.7633\n",
            "Epoch 34/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.7308\n",
            "Epoch 35/50\n",
            "40/40 [==============================] - 2s 43ms/step - loss: 0.6959\n",
            "Epoch 36/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.6741\n",
            "Epoch 37/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.6421\n",
            "Epoch 38/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.6119\n",
            "Epoch 39/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.5881\n",
            "Epoch 40/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.5596\n",
            "Epoch 41/50\n",
            "40/40 [==============================] - 2s 43ms/step - loss: 0.5313\n",
            "Epoch 42/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.5048\n",
            "Epoch 43/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.4898\n",
            "Epoch 44/50\n",
            "40/40 [==============================] - 2s 43ms/step - loss: 0.4680\n",
            "Epoch 45/50\n",
            "40/40 [==============================] - 2s 45ms/step - loss: 0.4417\n",
            "Epoch 46/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.4220\n",
            "Epoch 47/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.4038\n",
            "Epoch 48/50\n",
            "40/40 [==============================] - 2s 43ms/step - loss: 0.3825\n",
            "Epoch 49/50\n",
            "40/40 [==============================] - 2s 44ms/step - loss: 0.3632\n",
            "Epoch 50/50\n",
            "40/40 [==============================] - 2s 43ms/step - loss: 0.3502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfjLnAGKq1-J"
      },
      "source": [
        "**Step 7: Defining Inference Models**\r\n",
        "We create inference models which help in predicting translations.\r\n",
        "\r\n",
        "* Encoder inference model : Takes the English sentence as input and outputs LSTM states ( h and c ).\r\n",
        "\r\n",
        "* Decoder inference model : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the Marathi input seqeunces ( ones not having the <start> tag ). It will output the translations of the English sentence which we fed to the encoder model and its state values.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIkcV-sddFMq"
      },
      "source": [
        "def make_inference_models():\r\n",
        "    \r\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\r\n",
        "    \r\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 128 ,))\r\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 128 ,))\r\n",
        "    \r\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\r\n",
        "    \r\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\r\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\r\n",
        "    decoder_states = [state_h, state_c]\r\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\r\n",
        "    decoder_model = tf.keras.models.Model(\r\n",
        "        [decoder_inputs] + decoder_states_inputs,\r\n",
        "        [decoder_outputs] + decoder_states)\r\n",
        "    \r\n",
        "    return encoder_model , decoder_model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3UKpHxprRxR"
      },
      "source": [
        "**Step 8: Making some translations**\r\n",
        "1.   First, we take a English sequence and predict the state values using enc_model.\r\n",
        "2.   We set the state values in the decoder's LSTM.\r\n",
        "3.   Then, we generate a sequence which contains the <start> element.\r\n",
        "4.   We input this sequence in the dec_model.\r\n",
        "5.   We replace the <start> element with the element which was predicted by the dec_model and update the state values.\r\n",
        "6.   We carry out the above steps iteratively till we hit the <end> tag or the maximum sequence length.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiU_Z4XOdFJS"
      },
      "source": [
        "def str_to_tokens( sentence : str ):\r\n",
        "    words = sentence.lower().split()\r\n",
        "    tokens_list = list()\r\n",
        "    for word in words:\r\n",
        "        tokens_list.append( eng_word_dict[ word ] ) \r\n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKnkqzAKri92"
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\r\n",
        "\r\n",
        "for epoch in range( encoder_input_data.shape[0] ):\r\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter eng sentence : ' ) ) )\r\n",
        "    #states_values = enc_model.predict( encoder_input_data[ epoch ] )\r\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\r\n",
        "    empty_target_seq[0, 0] = mar_word_dict['start']\r\n",
        "    stop_condition = False\r\n",
        "    decoded_translation = ''\r\n",
        "    while not stop_condition :\r\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\r\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\r\n",
        "        sampled_word = None\r\n",
        "        for word , index in mar_word_dict.items() :\r\n",
        "            if sampled_word_index == index :\r\n",
        "                decoded_translation += ' {}'.format( word )\r\n",
        "                sampled_word = word\r\n",
        "        \r\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\r\n",
        "            stop_condition = True\r\n",
        "            \r\n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \r\n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\r\n",
        "        states_values = [ h , c ] \r\n",
        "\r\n",
        "    print( decoded_translation )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}